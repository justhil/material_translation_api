import logging
import httpx
import json
import traceback
from typing import Dict, Any, Optional, List

from app.core.config import settings

logger = logging.getLogger(__name__)


class LLMService:
    """å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ï¼Œè´Ÿè´£è°ƒç”¨è¯­è¨€æ¨¡å‹APIè¿›è¡Œç¿»è¯‘"""
    
    @staticmethod
    async def translate_text(
        text: str, 
        source_lang: str, 
        target_lang: str, 
        model: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç¿»è¯‘æ–‡æœ¬
        
        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            model: å¯é€‰çš„æ¨¡å‹åç§°ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è¯¥æ¨¡å‹è€Œä¸æ˜¯é»˜è®¤æ¨¡å‹
            
        Returns:
            åŒ…å«ç¿»è¯‘ç»“æœçš„å­—å…¸
        """
        if settings.CUSTOM_API_ENABLED:
            return await LLMService._translate_with_custom_api(text, source_lang, target_lang, model)
        else:
            return await LLMService._translate_with_default_api(text, source_lang, target_lang, model)
    
    @staticmethod
    async def _translate_with_default_api(
        text: str, 
        source_lang: str, 
        target_lang: str, 
        model: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨é»˜è®¤APIè¿›è¡Œç¿»è¯‘"""
        # ä½¿ç”¨å®é™…çš„APIè°ƒç”¨
        model_name = model or settings.LLM_MODEL_NAME
        
        logger.info(f"ä½¿ç”¨é»˜è®¤APIè¿›è¡Œç¿»è¯‘ï¼Œæ¨¡å‹: {model_name}")
        
        try:
            # æ„å»ºç³»ç»ŸæŒ‡ä»¤å’Œç”¨æˆ·æŒ‡ä»¤
            system_instruction = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘ç³»ç»Ÿï¼Œä¸“é—¨ä»{source_lang}è¯­è¨€ç¿»è¯‘åˆ°{target_lang}è¯­è¨€ã€‚è¯·åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å¤‡æ³¨ã€‚"
            user_instruction = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{source_lang}ç¿»è¯‘æˆ{target_lang}ï¼š\n\n{text}"
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                # å°è¯•ä½¿ç”¨ chat/completions ç«¯ç‚¹ï¼ˆä¼˜å…ˆï¼‰
                try:
                    response = await client.post(
                        f"{settings.LLM_API_BASE_URL}/v1/chat/completions",
                        headers={"Authorization": f"Bearer {settings.LLM_API_KEY}"},
                        json={
                            "model": model_name,
                            "messages": [
                                {"role": "system", "content": system_instruction},
                                {"role": "user", "content": user_instruction}
                            ],
                            "temperature": 0.3
                        }
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        translated_text = result["choices"][0]["message"]["content"].strip()
                    else:
                        # å¦‚æœ chat/completions å¤±è´¥ï¼Œå°è¯• completions ç«¯ç‚¹
                        logger.warning(f"Chat APIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Completions API: {response.status_code}")
                        response = await client.post(
                            f"{settings.LLM_API_BASE_URL}/v1/completions",
                            headers={"Authorization": f"Bearer {settings.LLM_API_KEY}"},
                            json={
                                "model": model_name,
                                "prompt": f"å°†ä»¥ä¸‹{source_lang}æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}:\n\n{text}",
                                "max_tokens": 2000,
                                "temperature": 0.3
                            }
                        )
                        
                        if response.status_code != 200:
                            logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                            return await LLMService._simulate_translation(text, source_lang, target_lang, model_name)
                        
                        result = response.json()
                        translated_text = result["choices"][0]["text"].strip()
                except Exception as api_error:
                    # å¦‚æœ chat/completions ç«¯ç‚¹ä¸å¯ç”¨ï¼Œå›é€€åˆ° completions ç«¯ç‚¹
                    logger.warning(f"Chat APIå¼‚å¸¸ï¼Œå°è¯•ä½¿ç”¨Completions API: {str(api_error)}")
                    response = await client.post(
                        f"{settings.LLM_API_BASE_URL}/v1/completions",
                        headers={"Authorization": f"Bearer {settings.LLM_API_KEY}"},
                        json={
                            "model": model_name,
                            "prompt": f"å°†ä»¥ä¸‹{source_lang}æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}:\n\n{text}",
                            "max_tokens": 2000,
                            "temperature": 0.3
                        }
                    )
                    
                    if response.status_code != 200:
                        logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                        return await LLMService._simulate_translation(text, source_lang, target_lang, model_name)
                    
                    result = response.json()
                    translated_text = result["choices"][0]["text"].strip()
                
                return {
                    "source_text": text,
                    "translated_text": translated_text,
                    "model_used": model_name
                }
        except Exception as e:
            logger.error(f"é»˜è®¤APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            # å¼‚å¸¸æ—¶ä½¿ç”¨æ¨¡æ‹Ÿç¿»è¯‘ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            return await LLMService._simulate_translation(text, source_lang, target_lang, model_name)
    
    @staticmethod
    async def _simulate_translation(
        text: str, 
        source_lang: str, 
        target_lang: str, 
        model_name: str
    ) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿç¿»è¯‘ç»“æœï¼ˆä»…ä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰"""
        logger.warning("ä½¿ç”¨æ¨¡æ‹Ÿç¿»è¯‘ä½œä¸ºåå¤‡æ–¹æ¡ˆ")
        
        if source_lang == 'zh' and target_lang == 'en':
            translated_text = f"[Translated to English]: {text}"
        elif source_lang == 'en' and target_lang == 'zh':
            translated_text = f"[ç¿»è¯‘æˆä¸­æ–‡]: {text}"
        else:
            translated_text = f"[Translated from {source_lang} to {target_lang}]: {text}"
        
        return {
            "source_text": text,
            "translated_text": translated_text,
            "model_used": model_name
        }
    
    @staticmethod
    async def _translate_with_custom_api(
        text: str, 
        source_lang: str, 
        target_lang: str, 
        model: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨è‡ªå®šä¹‰APIï¼ˆä¸OpenAIå…¼å®¹ï¼‰è¿›è¡Œç¿»è¯‘"""
        model_name = model or "gpt-3.5-turbo"
        base_url = settings.CUSTOM_API_BASE_URL.rstrip('/')
        api_version = settings.CUSTOM_API_VERSION.lstrip('/')
        
        # æ„å»ºç³»ç»ŸæŒ‡ä»¤å’Œç”¨æˆ·æŒ‡ä»¤
        system_instruction = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘ç³»ç»Ÿï¼Œä¸“é—¨ä»{source_lang}è¯­è¨€ç¿»è¯‘åˆ°{target_lang}è¯­è¨€ã€‚è¯·åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å¤‡æ³¨ã€‚"
        user_instruction = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{source_lang}ç¿»è¯‘æˆ{target_lang}ï¼š\n\n{text}"
        
        logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰APIè¿›è¡Œç¿»è¯‘ï¼Œæ¨¡å‹: {model_name}, APIåŸºç¡€URL: {base_url}")
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                # å°è¯•ä½¿ç”¨ chat/completions ç«¯ç‚¹ï¼ˆä¼˜å…ˆï¼‰
                try:
                    response = await client.post(
                        f"{base_url}/{api_version}/chat/completions",
                        headers={
                            "Authorization": f"Bearer {settings.CUSTOM_API_KEY}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": model_name,
                            "messages": [
                                {"role": "system", "content": system_instruction},
                                {"role": "user", "content": user_instruction}
                            ],
                            "temperature": 0.3
                        }
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        translated_text = result["choices"][0]["message"]["content"].strip()
                    else:
                        # å¦‚æœ chat/completions å¤±è´¥ï¼Œå°è¯• completions ç«¯ç‚¹
                        logger.warning(f"è‡ªå®šä¹‰Chat APIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Completions API: {response.status_code}")
                        response = await client.post(
                            f"{base_url}/{api_version}/completions",
                            headers={
                                "Authorization": f"Bearer {settings.CUSTOM_API_KEY}",
                                "Content-Type": "application/json"
                            },
                            json={
                                "model": model_name,
                                "prompt": f"å°†ä»¥ä¸‹{source_lang}æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}:\n\n{text}",
                                "max_tokens": 2000,
                                "temperature": 0.3
                            }
                        )
                        
                        if response.status_code != 200:
                            logger.error(f"è‡ªå®šä¹‰APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                            # å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤APIä½œä¸ºåå¤‡æ–¹æ¡ˆ
                            logger.info("å°è¯•ä½¿ç”¨é»˜è®¤APIä½œä¸ºåå¤‡")
                            return await LLMService._translate_with_default_api(text, source_lang, target_lang, model)
                        
                        result = response.json()
                        translated_text = result["choices"][0]["text"].strip()
                except Exception as api_error:
                    # å¦‚æœ chat/completions ç«¯ç‚¹ä¸å¯ç”¨ï¼Œå›é€€åˆ° completions ç«¯ç‚¹
                    logger.warning(f"è‡ªå®šä¹‰Chat APIå¼‚å¸¸ï¼Œå°è¯•ä½¿ç”¨Completions API: {str(api_error)}")
                    response = await client.post(
                        f"{base_url}/{api_version}/completions",
                        headers={
                            "Authorization": f"Bearer {settings.CUSTOM_API_KEY}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": model_name,
                            "prompt": f"å°†ä»¥ä¸‹{source_lang}æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}:\n\n{text}",
                            "max_tokens": 2000,
                            "temperature": 0.3
                        }
                    )
                    
                    if response.status_code != 200:
                        logger.error(f"è‡ªå®šä¹‰APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                        # å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤APIä½œä¸ºåå¤‡æ–¹æ¡ˆ
                        logger.info("å°è¯•ä½¿ç”¨é»˜è®¤APIä½œä¸ºåå¤‡")
                        return await LLMService._translate_with_default_api(text, source_lang, target_lang, model)
                    
                    result = response.json()
                    translated_text = result["choices"][0]["text"].strip()
                
                return {
                    "source_text": text,
                    "translated_text": translated_text,
                    "model_used": model_name
                }
        except Exception as e:
            logger.error(f"è‡ªå®šä¹‰APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            # å‘ç”Ÿå¼‚å¸¸æ—¶ä½¿ç”¨é»˜è®¤APIä½œä¸ºåå¤‡æ–¹æ¡ˆ
            logger.info("å°è¯•ä½¿ç”¨é»˜è®¤APIä½œä¸ºåå¤‡")
            return await LLMService._translate_with_default_api(text, source_lang, target_lang, model)
    
    @staticmethod
    async def get_available_models() -> List[Dict[str, str]]:
        """
        è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        ä»APIè·å–å¯ç”¨çš„è¯­è¨€æ¨¡å‹åˆ—è¡¨ï¼Œå§‹ç»ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„APIé…ç½®
        
        Returns:
            å¯ç”¨æ¨¡å‹çš„åˆ—è¡¨
        """
        logger.info(f"è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ŒAPIé…ç½®: å¯ç”¨è‡ªå®šä¹‰API={settings.CUSTOM_API_ENABLED}")
        
        if settings.CUSTOM_API_ENABLED and settings.CUSTOM_API_KEY:
            # ä½¿ç”¨è‡ªå®šä¹‰APIè·å–æ¨¡å‹åˆ—è¡¨
            try:
                models = await LLMService._fetch_custom_api_models()
                if models:
                    logger.info(f"æˆåŠŸä»è‡ªå®šä¹‰APIè·å–æ¨¡å‹åˆ—è¡¨: {len(models)}")
                    return models
                else:
                    logger.warning("ä»è‡ªå®šä¹‰APIè·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥")
                    # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯é»˜è®¤åˆ—è¡¨
                    return []
            except Exception as e:
                logger.error(f"è·å–è‡ªå®šä¹‰APIæ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {str(e)}")
                # å‡ºç°å¼‚å¸¸ä¹Ÿè¿”å›ç©ºåˆ—è¡¨
                return []
        else:
            # APIæœªå¯ç”¨ï¼Œè¿”å›ç©ºåˆ—è¡¨
            logger.warning("è‡ªå®šä¹‰APIæœªå¯ç”¨æˆ–APIå¯†é’¥æœªè®¾ç½®ï¼Œæ— æ³•è·å–æ¨¡å‹åˆ—è¡¨")
            return []
    
    @staticmethod
    async def _fetch_custom_api_models() -> List[Dict[str, str]]:
        """ä»è‡ªå®šä¹‰APIè·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        base_url = settings.CUSTOM_API_BASE_URL.rstrip('/')
        api_version = settings.CUSTOM_API_VERSION.lstrip('/')
        api_url = f"{base_url}/{api_version}/models"
        
        logger.info(f"ğŸŒ æ­£åœ¨ä» {api_url} è·å–æ¨¡å‹åˆ—è¡¨")
        
        try:
            headers = {"Authorization": f"Bearer {settings.CUSTOM_API_KEY}"}
            logger.info(f"è¯·æ±‚å¤´: {headers}")
            
            timeout = httpx.Timeout(30.0, connect=10.0)
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.get(api_url, headers=headers)
                
                # è®°å½•åŸå§‹å“åº”
                logger.info(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
                logger.info(f"APIå“åº”å¤´: {response.headers}")
                
                if response.status_code != 200:
                    logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code} - {response.text}")
                    return []
                
                # è§£æå“åº”å†…å®¹
                try:
                    result = response.json()
                    logger.info(f"APIå“åº”å†…å®¹: {json.dumps(result, ensure_ascii=False)[:200]}...")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ è§£æAPIå“åº”JSONå¤±è´¥: {str(e)}")
                    logger.error(f"å“åº”å†…å®¹: {response.text[:200]}...")
                    return []
                
                models = []
                
                # æ£€æŸ¥APIå“åº”æ ¼å¼
                if "data" not in result:
                    logger.warning("âš ï¸ APIå“åº”ä¸­æœªæ‰¾åˆ°'data'å­—æ®µ")
                    if isinstance(result, list):
                        logger.info("APIå“åº”æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå°è¯•ç›´æ¥å¤„ç†")
                        model_list = result
                    else:
                        logger.error("APIå“åº”æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                        return []
                else:
                    model_list = result.get("data", [])
                
                # å¤„ç†æ¨¡å‹åˆ—è¡¨
                for model_data in model_list:
                    # æ—¥å¿—è®°å½•è¯¥æ¨¡å‹çš„åŸå§‹æ•°æ®
                    logger.info(f"å¤„ç†æ¨¡å‹æ•°æ®: {json.dumps(model_data, ensure_ascii=False)}")
                    
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™ç›´æ¥ä½œä¸ºIDä½¿ç”¨
                    if isinstance(model_data, str):
                        model_id = model_data
                    # å¦åˆ™ï¼Œå°è¯•ä»å¯¹è±¡ä¸­æå–ID
                    else:
                        model_id = model_data.get("id", "")
                        if not model_id and "name" in model_data:
                            model_id = model_data.get("name", "")
                    
                    if not model_id:
                        logger.warning(f"âš ï¸ æ— æ³•ä»æ•°æ®ä¸­æå–æ¨¡å‹ID: {model_data}")
                        continue
                    
                    # ç­›é€‰å¸¸è§çš„LLMæ¨¡å‹
                    if (model_id and (
                            "gpt" in model_id.lower() or 
                            "llama" in model_id.lower() or 
                            "claude" in model_id.lower() or 
                            "text-embedding" in model_id.lower() or
                            "gemini" in model_id.lower() or
                            "deepseek" in model_id.lower())):
                        models.append({
                            "id": model_id,
                            "name": model_id,
                            "type": "custom"
                        })
                
                logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(models)} ä¸ªæ¨¡å‹")
                return models
        except httpx.RequestError as e:
            logger.error(f"âŒ HTTPè¯·æ±‚å¼‚å¸¸: {str(e)}")
            return []
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {str(e)}")
            logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return []
    
    @staticmethod
    async def get_ai_completion(prompt: str) -> str:
        """
        ä½¿ç”¨AIç”Ÿæˆæ–‡æœ¬è¡¥å…¨
        
        Args:
            prompt: æç¤ºè¯­
            
        Returns:
            AIç”Ÿæˆçš„æ–‡æœ¬
        """
        logger.info("è°ƒç”¨AIè¿›è¡Œæ–‡æœ¬è¡¥å…¨")
        
        try:
            if settings.CUSTOM_API_ENABLED and settings.CUSTOM_API_KEY:
                # ä½¿ç”¨è‡ªå®šä¹‰API
                base_url = settings.CUSTOM_API_BASE_URL.rstrip('/')
                api_version = settings.CUSTOM_API_VERSION.lstrip('/')
                model_name = "gpt-3.5-turbo"  # é»˜è®¤ä½¿ç”¨è¾ƒå¿«çš„æ¨¡å‹
                
                async with httpx.AsyncClient(timeout=60.0) as client:
                    response = await client.post(
                        f"{base_url}/{api_version}/chat/completions",
                        headers={
                            "Authorization": f"Bearer {settings.CUSTOM_API_KEY}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": model_name,
                            "messages": [
                                {"role": "user", "content": prompt}
                            ],
                            "temperature": 0.3
                        }
                    )
                    
                    if response.status_code != 200:
                        logger.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                        return "{}"  # è¿”å›ç©ºJSON
                    
                    result = response.json()
                    completion = result["choices"][0]["message"]["content"].strip()
                    return completion
            else:
                # ç®€å•æ¨¡æ‹Ÿè¿”å›
                logger.warning("æœªå¯ç”¨è‡ªå®šä¹‰APIï¼Œæ— æ³•è¿›è¡ŒAIæœ¯è¯­æå–")
                return "{}"  # è¿”å›ç©ºJSON
                
        except Exception as e:
            logger.error(f"AIè¡¥å…¨å¼‚å¸¸: {str(e)}")
            return "{}"  # è¿”å›ç©ºJSON
        
llm_service = LLMService() 